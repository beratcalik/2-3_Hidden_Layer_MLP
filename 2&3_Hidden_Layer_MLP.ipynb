{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c116952f-3104-4f3a-b1f2-6ebcd211bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e90de79-4c0c-4de4-b6e6-edf9ccafeca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"BankNote_Authentication.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94788180-53c0-472f-a297-2ed51eacd032",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac = 1, random_state = 42).reset_index(drop= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0b72ecb-f7da-4e59-9592-e5a9bc34ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.iloc[:, :-1], data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abf15117-791b-462c-9e95-45977a017cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X type is <class 'pandas.core.frame.DataFrame'>\n",
      "X shape is (1372, 4)\n",
      "y type is <class 'pandas.core.series.Series'>\n",
      "y shape is (1372,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X type is \" + str(type(X)))\n",
    "print(\"X shape is \" + str(X.shape))\n",
    "print(\"y type is \" + str(type(y)))\n",
    "print(\"y shape is \" + str(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57762568-8313-4604-b7aa-b811143423f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d39b717d-b5ac-4326-af98-762806ff7699",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf6ace47-1c69-4eda-959a-73b5c9577a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X type is <class 'numpy.ndarray'>\n",
      "X shape is (1372, 4)\n",
      "y type is <class 'numpy.ndarray'>\n",
      "y shape is (1372, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"X type is \" + str(type(X)))\n",
    "print(\"X shape is \" + str(X.shape))\n",
    "print(\"y type is \" + str(type(y)))\n",
    "print(\"y shape is \" + str(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5baf1860-08bb-4668-be29-9413c6b80512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a0246cb0-b8f9-4e16-a3bb-406a768b2a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b7f914b-07f9-4506-8da9-af432ade3bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape is (1097, 4)\n",
      "X test shape is (275, 4)\n",
      "y train shape is (1097, 1)\n",
      "y test shape is (275, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"X train shape is \" + str(X_train.shape))\n",
    "print(\"X test shape is \" + str(X_test.shape))\n",
    "print(\"y train shape is \" + str(y_train.shape))\n",
    "print(\"y test shape is \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eefe8123-13c6-475a-9c1c-d261846c9cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    # tanh türevi = 1 - tanh(z)^2\n",
    "    return 1 - np.tanh(z)**2\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ea3e720-2d08-4299-ac97-fc391b268c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(activation_name):\n",
    "    if activation_name == 'tanh':\n",
    "        return tanh, tanh_derivative\n",
    "    elif activation_name == 'relu':\n",
    "        return relu, relu_derivative\n",
    "    else:\n",
    "        raise ValueError(\"Geçersiz aktivasyon fonksiyonu ismi!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6a63e54f-7c8d-43b8-985d-fbafc1f13c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, layer_sizes, \n",
    "                 activation_hidden='tanh', \n",
    "                 activation_output='sigmoid', \n",
    "                 learning_rate=0.01, \n",
    "                 seed=42):\n",
    "        \"\"\"\n",
    "        layer_sizes: Örneğin [4, 5, 1] => 4 giriş, 5 gizli nöron, 1 çıkış nöronu\n",
    "        activation_hidden: 'tanh' veya 'relu'\n",
    "        activation_output: genelde 'sigmoid'\n",
    "        learning_rate: Öğrenme oranı\n",
    "        seed: Rastgele ağırlık başlatma için seed\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Gizli katman aktivasyonu\n",
    "        self.hidden_activation, self.hidden_activation_deriv = get_activation(activation_hidden)\n",
    "        \n",
    "        # Çıkış katmanı için aktivasyon fonksiyonu (genelde sigmoid)\n",
    "        if activation_output == 'sigmoid':\n",
    "            self.output_activation = lambda z: 1 / (1 + np.exp(-z))\n",
    "            self.output_activation_deriv = lambda z: self.output_activation(z) * (1 - self.output_activation(z))\n",
    "        else:\n",
    "            raise ValueError(\"Çıkış katmanı için bu örnekte sadece sigmoid destekleniyor.\")\n",
    "        \n",
    "        # Ağırlık ve bias başlatma\n",
    "        self.parameters = {}\n",
    "        self._init_params()\n",
    "    \n",
    "    def _init_params(self):\n",
    "        # layer_sizes örn: [4, 5, 1]\n",
    "        for i in range(len(self.layer_sizes) - 1):\n",
    "            in_dim = self.layer_sizes[i]\n",
    "            out_dim = self.layer_sizes[i+1]\n",
    "            # Xavier veya He init isterseniz ekleyebilirsiniz. Burada basit random normal kullandık.\n",
    "            self.parameters[f\"W{i+1}\"] = np.random.randn(in_dim, out_dim) * 0.01\n",
    "            self.parameters[f\"b{i+1}\"] = np.zeros((1, out_dim))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        İleri yayılım.\n",
    "        returns:\n",
    "          - activations: her katmandaki 'a' (aktivasyon)\n",
    "          - zs: her katmandaki 'z' (lineer kısım)\n",
    "        \"\"\"\n",
    "        activations = [X]\n",
    "        zs = []\n",
    "        \n",
    "        # Kaç katman var (örneğin [4,5,5,1] => 3 adet W ve b seti)\n",
    "        L = len(self.layer_sizes) - 1\n",
    "        \n",
    "        for i in range(1, L+1):\n",
    "            W = self.parameters[f\"W{i}\"]\n",
    "            b = self.parameters[f\"b{i}\"]\n",
    "            \n",
    "            # Önceki katmanın aktivasyonu\n",
    "            A_prev = activations[-1]\n",
    "            \n",
    "            z = A_prev.dot(W) + b\n",
    "            zs.append(z)\n",
    "            \n",
    "            if i < L:\n",
    "                # Gizli katman aktivasyonu\n",
    "                a = self.hidden_activation(z)\n",
    "            else:\n",
    "                # Çıkış katmanı aktivasyonu (sigmoid)\n",
    "                a = self.output_activation(z)\n",
    "            \n",
    "            activations.append(a)\n",
    "        \n",
    "        return activations, zs\n",
    "    \n",
    "    def backward(self, activations, zs, y):\n",
    "        \"\"\"\n",
    "        Geri yayılım (backprop).\n",
    "        y shape = (batch_size, )\n",
    "        \"\"\"\n",
    "        grads = {}\n",
    "        m = y.shape[0]\n",
    "        L = len(self.layer_sizes) - 1\n",
    "        \n",
    "        # Çıkış katmanı aktivasyonu (sigmoid) için hata hesabı\n",
    "        # a[L] => output\n",
    "        aL = activations[-1]\n",
    "        \n",
    "        # Binary cross-entropy için dA = -(y/a - (1-y)/(1-a))\n",
    "        # pratikte a[L] - y olarak da sıkça kullanılıyor (sigmoid cross-entropy yaklaşımı)\n",
    "        dA = -(np.divide(y, aL) - np.divide(1 - y, 1 - aL))\n",
    "        \n",
    "        for i in reversed(range(1, L+1)):\n",
    "            z = zs[i-1]\n",
    "            W = self.parameters[f\"W{i}\"]\n",
    "            \n",
    "            if i == L:\n",
    "                # Çıkış katmanı türevi\n",
    "                dZ = dA * self.output_activation_deriv(z)\n",
    "            else:\n",
    "                # Gizli katman türevi\n",
    "                dZ = dA * self.hidden_activation_deriv(z)\n",
    "            \n",
    "            A_prev = activations[i-1]\n",
    "            \n",
    "            # dW = (1/m) * A_prev.T.dot(dZ)\n",
    "            dW = (1/m) * A_prev.T.dot(dZ)\n",
    "            \n",
    "            # db = (1/m) * sum(dZ)\n",
    "            db = (1/m) * np.sum(dZ, axis=0, keepdims=True)\n",
    "            \n",
    "            grads[f\"dW{i}\"] = dW\n",
    "            grads[f\"db{i}\"] = db\n",
    "            \n",
    "            # Bir önceki katmana ait dA hesabı\n",
    "            dA = dZ.dot(W.T)\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def update_params(self, grads):\n",
    "        \"\"\"\n",
    "        Stokastik gradyan inişi (SGD) ile parametre güncellemesi\n",
    "        \"\"\"\n",
    "        L = len(self.layer_sizes) - 1\n",
    "        for i in range(1, L+1):\n",
    "            self.parameters[f\"W{i}\"] -= self.learning_rate * grads[f\"dW{i}\"]\n",
    "            self.parameters[f\"b{i}\"] -= self.learning_rate * grads[f\"db{i}\"]\n",
    "    \n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Binary Cross-Entropy Loss\n",
    "        y_pred ve y_true (0 veya 1) boyutları (m,)\n",
    "        \"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        # epsilon ile sayısal kararlılık sağlanabilir\n",
    "        epsilon = 1e-9\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        loss = - (1/m) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "    \n",
    "    def fit(self, X, y, n_steps=1000):\n",
    "        \"\"\"\n",
    "        Modeli n_steps boyunca eğitir.\n",
    "        \"\"\"\n",
    "        loss_history = []\n",
    "        for step in range(n_steps):\n",
    "            # forward\n",
    "            activations, zs = self.forward(X)\n",
    "            y_pred = activations[-1].reshape(-1)  # (m,)\n",
    "            \n",
    "            # loss\n",
    "            loss = self.compute_loss(y_pred, y)\n",
    "            loss_history.append(loss)\n",
    "            \n",
    "            # backward\n",
    "            grads = self.backward(activations, zs, y)\n",
    "            \n",
    "            # update\n",
    "            self.update_params(grads)\n",
    "            \n",
    "            # İsterseniz her 100 adımda bir ekrana yazdırabilirsiniz\n",
    "            if (step+1) % 100 == 0:\n",
    "                print(f\"Step {step+1}/{n_steps}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        return loss_history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        İleri yayılım yaparak çıktı tahmini (0 veya 1)\n",
    "        \"\"\"\n",
    "        activations, _ = self.forward(X)\n",
    "        y_prob = activations[-1]\n",
    "        # 0.5 eşik\n",
    "        return (y_prob >= 0.5).astype(int).reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "edcaa7c4-8355-4bb3-8868-b4f76fe603c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100/1000, Loss: 757.5739\n",
      "Step 200/1000, Loss: 764.7127\n",
      "Step 300/1000, Loss: 817.5907\n",
      "Step 400/1000, Loss: 937.4890\n",
      "Step 500/1000, Loss: 1079.5902\n",
      "Step 600/1000, Loss: 1211.9188\n",
      "Step 700/1000, Loss: 1328.4566\n",
      "Step 800/1000, Loss: 1430.6122\n",
      "Step 900/1000, Loss: 1520.9984\n",
      "Step 1000/1000, Loss: 1601.9375\n",
      "Step 100/1000, Loss: 757.7736\n",
      "Step 200/1000, Loss: 756.1716\n",
      "Step 300/1000, Loss: 755.1999\n",
      "Step 400/1000, Loss: 754.6101\n",
      "Step 500/1000, Loss: 754.2519\n",
      "Step 600/1000, Loss: 754.0341\n",
      "Step 700/1000, Loss: 753.9017\n",
      "Step 800/1000, Loss: 753.8211\n",
      "Step 900/1000, Loss: 753.7720\n",
      "Step 1000/1000, Loss: 753.7421\n",
      "Step 100/1000, Loss: 757.6439\n",
      "Step 200/1000, Loss: 762.4838\n",
      "Step 300/1000, Loss: 820.7771\n",
      "Step 400/1000, Loss: 959.9543\n",
      "Step 500/1000, Loss: 1129.6962\n",
      "Step 600/1000, Loss: 1285.9215\n",
      "Step 700/1000, Loss: 1421.1396\n",
      "Step 800/1000, Loss: 1537.9491\n",
      "Step 900/1000, Loss: 1640.1264\n",
      "Step 1000/1000, Loss: 1730.8255\n",
      "Step 100/1000, Loss: 757.7727\n",
      "Step 200/1000, Loss: 756.1705\n",
      "Step 300/1000, Loss: 755.1989\n",
      "Step 400/1000, Loss: 754.6093\n",
      "Step 500/1000, Loss: 754.2512\n",
      "Step 600/1000, Loss: 754.0335\n",
      "Step 700/1000, Loss: 753.9012\n",
      "Step 800/1000, Loss: 753.8207\n",
      "Step 900/1000, Loss: 753.7717\n",
      "Step 1000/1000, Loss: 753.7419\n",
      "Step 100/1000, Loss: 757.5048\n",
      "Step 200/1000, Loss: 755.1830\n",
      "Step 300/1000, Loss: 767.9355\n",
      "Step 400/1000, Loss: 847.3971\n",
      "Step 500/1000, Loss: 999.7981\n",
      "Step 600/1000, Loss: 1160.4058\n",
      "Step 700/1000, Loss: 1308.9173\n",
      "Step 800/1000, Loss: 1456.1752\n",
      "Step 900/1000, Loss: 1596.4643\n",
      "Step 1000/1000, Loss: 1729.9344\n",
      "Step 100/1000, Loss: 757.7742\n",
      "Step 200/1000, Loss: 756.1725\n",
      "Step 300/1000, Loss: 755.2008\n",
      "Step 400/1000, Loss: 754.6108\n",
      "Step 500/1000, Loss: 754.2524\n",
      "Step 600/1000, Loss: 754.0345\n",
      "Step 700/1000, Loss: 753.9019\n",
      "Step 800/1000, Loss: 753.8213\n",
      "Step 900/1000, Loss: 753.7722\n",
      "Step 1000/1000, Loss: 753.7423\n",
      "Step 100/1000, Loss: 757.7881\n",
      "Step 200/1000, Loss: 756.1220\n",
      "Step 300/1000, Loss: 755.0218\n",
      "Step 400/1000, Loss: 773.9374\n",
      "Step 500/1000, Loss: 891.2537\n",
      "Step 600/1000, Loss: 1099.0303\n",
      "Step 700/1000, Loss: 1302.6911\n",
      "Step 800/1000, Loss: 1483.7809\n",
      "Step 900/1000, Loss: 1643.2944\n",
      "Step 1000/1000, Loss: 1783.2059\n",
      "Step 100/1000, Loss: 757.7741\n",
      "Step 200/1000, Loss: 756.1723\n",
      "Step 300/1000, Loss: 755.2007\n",
      "Step 400/1000, Loss: 754.6108\n",
      "Step 500/1000, Loss: 754.2525\n",
      "Step 600/1000, Loss: 754.0346\n",
      "Step 700/1000, Loss: 753.9020\n",
      "Step 800/1000, Loss: 753.8214\n",
      "Step 900/1000, Loss: 753.7722\n",
      "Step 1000/1000, Loss: 753.7423\n",
      "     Model Activation  n_hidden  Accuracy    FinalLoss  n_steps\n",
      "0  2-Layer       tanh         5  0.981818  1601.937498     1000\n",
      "1  3-Layer       tanh         5  0.556364   753.742145     1000\n",
      "2  2-Layer       tanh        10  0.981818  1730.825450     1000\n",
      "3  3-Layer       tanh        10  0.556364   753.741898     1000\n",
      "4  2-Layer       relu         5  0.985455  1729.934360     1000\n",
      "5  3-Layer       relu         5  0.556364   753.742262     1000\n",
      "6  2-Layer       relu        10  0.985455  1783.205886     1000\n",
      "7  3-Layer       relu        10  0.556364   753.742329     1000\n"
     ]
    }
   ],
   "source": [
    "n_hidden_candidates = [5, 10]  # örnek\n",
    "n_steps = 1000\n",
    "\n",
    "results = []  # (model_name, activation, n_hidden, final_accuracy, final_loss, n_steps)\n",
    "\n",
    "for activation_func in ['tanh', 'relu']:\n",
    "    for n_hidden in n_hidden_candidates:\n",
    "        # 2-Layer\n",
    "        mlp_2layer = MLP(layer_sizes=[X_train.shape[1], n_hidden, 1],\n",
    "                 activation_hidden=activation_func,\n",
    "                 activation_output='sigmoid',\n",
    "                 learning_rate=0.01,\n",
    "                 seed=42)\n",
    "\n",
    "        \n",
    "        loss_hist_2layer = mlp_2layer.fit(X_train, y_train, n_steps=n_steps)\n",
    "        \n",
    "        y_pred_2layer = mlp_2layer.predict(X_test)\n",
    "        acc_2layer = accuracy_score(y_test, y_pred_2layer)\n",
    "        final_loss_2layer = loss_hist_2layer[-1]\n",
    "        \n",
    "        results.append((\"2-Layer\", activation_func, n_hidden, acc_2layer, final_loss_2layer, n_steps))\n",
    "        \n",
    "        # 3-Layer\n",
    "        mlp_3layer = MLP(layer_sizes=[X_train.shape[1], n_hidden, n_hidden, 1],\n",
    "                         activation_hidden=activation_func,\n",
    "                         activation_output='sigmoid',\n",
    "                         learning_rate=0.01,\n",
    "                         seed=42)\n",
    "        \n",
    "        loss_hist_3layer = mlp_3layer.fit(X_train, y_train, n_steps=n_steps)\n",
    "        \n",
    "        y_pred_3layer = mlp_3layer.predict(X_test)\n",
    "        acc_3layer = accuracy_score(y_test, y_pred_3layer)\n",
    "        final_loss_3layer = loss_hist_3layer[-1]\n",
    "        \n",
    "        results.append((\"3-Layer\", activation_func, n_hidden, acc_3layer, final_loss_3layer, n_steps))\n",
    "\n",
    "# Sonuçları inceleyelim\n",
    "df_results = pd.DataFrame(results, columns=[\"Model\", \"Activation\", \"n_hidden\", \"Accuracy\", \"FinalLoss\", \"n_steps\"])\n",
    "print(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2a9f18da-d0b3-43f1-8d14-c1024f5c79a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100/1000, Loss: 757.5048\n",
      "Step 200/1000, Loss: 755.1830\n",
      "Step 300/1000, Loss: 767.9355\n",
      "Step 400/1000, Loss: 847.3971\n",
      "Step 500/1000, Loss: 999.7981\n",
      "Step 600/1000, Loss: 1160.4058\n",
      "Step 700/1000, Loss: 1308.9173\n",
      "Step 800/1000, Loss: 1456.1752\n",
      "Step 900/1000, Loss: 1596.4643\n",
      "Step 1000/1000, Loss: 1729.9344\n",
      "Confusion Matrix:\n",
      " [[149   4]\n",
      " [  0 122]]\n",
      "Accuracy: 0.9855\n",
      "Precision: 0.9683\n",
      "Recall: 1.0000\n",
      "F1-Score: 0.9839\n"
     ]
    }
   ],
   "source": [
    "# Örnek olarak 2-Layer + ReLU, n_hidden=5 modelinin seçildiğini varsayalım:\n",
    "best_model = MLP(layer_sizes=[X_train.shape[1], 5, 1],\n",
    "                 activation_hidden='relu',\n",
    "                 activation_output='sigmoid',\n",
    "                 learning_rate=0.01,\n",
    "                 seed=42)\n",
    "\n",
    "best_model.fit(X_train, y_train, n_steps=1000)\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "acc = accuracy_score(y_test, y_pred_best)\n",
    "prec = precision_score(y_test, y_pred_best)\n",
    "rec = recall_score(y_test, y_pred_best)\n",
    "f1 = f1_score(y_test, y_pred_best)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall: {rec:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "989abf0c-defe-4a14-bf72-8097cd5cf42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berat\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn 2-Layer MLP Accuracy: 0.9963636363636363\n",
      "Scikit-learn 3-Layer MLP Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berat\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1098: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Örneğin 2-Layer, tanh aktivasyon, 5 nöron\n",
    "mlp_sklearn_2layer = MLPClassifier(hidden_layer_sizes=(5,),\n",
    "                                   activation='tanh',\n",
    "                                   solver='sgd',\n",
    "                                   learning_rate_init=0.01,\n",
    "                                   max_iter=1000,\n",
    "                                   random_state=42)\n",
    "\n",
    "mlp_sklearn_2layer.fit(X_train, y_train)\n",
    "y_pred_sklearn_2layer = mlp_sklearn_2layer.predict(X_test)\n",
    "\n",
    "print(\"Scikit-learn 2-Layer MLP Accuracy:\", accuracy_score(y_test, y_pred_sklearn_2layer))\n",
    "\n",
    "# Örneğin 3-Layer, ReLU aktivasyon, 5 + 5 nöron\n",
    "mlp_sklearn_3layer = MLPClassifier(hidden_layer_sizes=(5, 5),\n",
    "                                   activation='relu',\n",
    "                                   solver='sgd',\n",
    "                                   learning_rate_init=0.01,\n",
    "                                   max_iter=1000,\n",
    "                                   random_state=42)\n",
    "\n",
    "mlp_sklearn_3layer.fit(X_train, y_train)\n",
    "y_pred_sklearn_3layer = mlp_sklearn_3layer.predict(X_test)\n",
    "\n",
    "print(\"Scikit-learn 3-Layer MLP Accuracy:\", accuracy_score(y_test, y_pred_sklearn_3layer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fa395e56-9349-48b4-9088-2aec464c5bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100/1000, Loss: 0.5150\n",
      "Step 200/1000, Loss: 0.3229\n",
      "Step 300/1000, Loss: 0.1905\n",
      "Step 400/1000, Loss: 0.1251\n",
      "Step 500/1000, Loss: 0.0910\n",
      "Step 600/1000, Loss: 0.0712\n",
      "Step 700/1000, Loss: 0.0583\n",
      "Step 800/1000, Loss: 0.0492\n",
      "Step 900/1000, Loss: 0.0424\n",
      "Step 1000/1000, Loss: 0.0372\n",
      "PyTorch 3-Layer MLP Accuracy: 0.9818181818181818\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Torch tensörlerine dönüştürme\n",
    "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "X_test_torch = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_torch = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# 3-Layer MLP (2 gizli katman)\n",
    "class MLP_Torch(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, activation='relu'):\n",
    "        super(MLP_Torch, self).__init__()\n",
    "        if activation == 'relu':\n",
    "            self.act = nn.ReLU()\n",
    "        elif activation == 'tanh':\n",
    "            self.act = nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError(\"Aktivasyon fonksiyonu bilinmiyor!\")\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.layer1(x))\n",
    "        x = self.act(self.layer2(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "model_torch = MLP_Torch(input_dim=X_train.shape[1], hidden_dim=5, activation='relu')\n",
    "criterion = nn.BCELoss()   # Binary Cross Entropy\n",
    "optimizer = optim.SGD(model_torch.parameters(), lr=0.01)\n",
    "\n",
    "# Eğitim döngüsü\n",
    "n_steps = 1000\n",
    "for step in range(n_steps):\n",
    "    # ileri yayılım\n",
    "    y_pred = model_torch(X_train_torch)\n",
    "    loss = criterion(y_pred, y_train_torch)\n",
    "    \n",
    "    # geri yayılım\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (step+1) % 100 == 0:\n",
    "        print(f\"Step {step+1}/{n_steps}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test aşaması\n",
    "model_torch.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model_torch(X_test_torch)\n",
    "    # 0.5 eşiği\n",
    "    y_test_pred_class = (y_test_pred >= 0.5).float()\n",
    "    \n",
    "acc_torch = accuracy_score(y_test, y_test_pred_class.numpy())\n",
    "print(\"PyTorch 3-Layer MLP Accuracy:\", acc_torch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2efe85b6-352a-4627-974c-33aef9323bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100/1000, Loss: 0.5336\n",
      "Step 200/1000, Loss: 0.3930\n",
      "Step 300/1000, Loss: 0.2779\n",
      "Step 400/1000, Loss: 0.2061\n",
      "Step 500/1000, Loss: 0.1595\n",
      "Step 600/1000, Loss: 0.1296\n",
      "Step 700/1000, Loss: 0.1095\n",
      "Step 800/1000, Loss: 0.0949\n",
      "Step 900/1000, Loss: 0.0839\n",
      "Step 1000/1000, Loss: 0.0752\n",
      "PyTorch 2-Layer MLP Accuracy: 0.9854545454545455\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Torch tensörlerine dönüştürme\n",
    "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "X_test_torch = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_torch = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# 2-Layer MLP (1 gizli katman)\n",
    "class MLP_Torch_2Layer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, activation='relu'):\n",
    "        super(MLP_Torch_2Layer, self).__init__()\n",
    "        # Aktivasyon fonksiyonunun seçimi\n",
    "        if activation == 'relu':\n",
    "            self.act = nn.ReLU()\n",
    "        elif activation == 'tanh':\n",
    "            self.act = nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError(\"Aktivasyon fonksiyonu bilinmiyor!\")\n",
    "        \n",
    "        # 1 gizli katman ve 1 çıkış katmanı\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.layer1(x))   # Gizli katman aktivasyonu\n",
    "        x = self.sigmoid(self.output(x)) # Çıkış katmanı (sigmoid)\n",
    "        return x\n",
    "\n",
    "# Modeli oluşturma\n",
    "model_torch_2layer = MLP_Torch_2Layer(input_dim=X_train.shape[1], hidden_dim=5, activation='relu')\n",
    "\n",
    "# Kayıp fonksiyonu ve optimizasyon (SGD)\n",
    "criterion = nn.BCELoss()   # Binary Cross-Entropy Loss\n",
    "optimizer = optim.SGD(model_torch_2layer.parameters(), lr=0.01)\n",
    "\n",
    "# Eğitim döngüsü\n",
    "n_steps = 1000\n",
    "for step in range(n_steps):\n",
    "    # İleri yayılım\n",
    "    y_pred = model_torch_2layer(X_train_torch)\n",
    "    loss = criterion(y_pred, y_train_torch)\n",
    "    \n",
    "    # Geri yayılım ve parametre güncelleme\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (step+1) % 100 == 0:\n",
    "        print(f\"Step {step+1}/{n_steps}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test aşaması\n",
    "model_torch_2layer.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model_torch_2layer(X_test_torch)\n",
    "    # 0.5 eşiği ile sınıflandırma\n",
    "    y_test_pred_class = (y_test_pred >= 0.5).float()\n",
    "\n",
    "# Test accuracy hesaplama\n",
    "acc_torch_2layer = accuracy_score(y_test, y_test_pred_class.numpy())\n",
    "print(\"PyTorch 2-Layer MLP Accuracy:\", acc_torch_2layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278ec694-19d7-4fa4-b8c0-2fac09fe1226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
